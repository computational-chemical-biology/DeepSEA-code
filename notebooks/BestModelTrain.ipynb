{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "%matplotlib inline\n",
    "sn.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDM = [60,50,50]\n",
    "NCRD = [70,50,60]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "CNN_DIM = HDM[0]\n",
    "CNNS_DIM = HMD[1]\n",
    "GRU_DIM = HDM[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a respective datasets and encoder\n",
    "ENCODER = joblib.load(\"../data/NCRD/ncrd95-uniprot-ma_clf_enc.joblib\")\n",
    "OUTDIM = len(ENCODER.categories_[0]) #Dense layer number of units\n",
    "TRAIN = \"../data/NCRD/TrainNcrd95-ma-align.csv\"\n",
    "TEST = \"../data/NCRD/TestNcrd95-ma-align.csv\"\n",
    "OUTPUTDIR = \"../models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER.categories_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"cpu\"):\n",
    "    train = pd.read_csv(TRAIN)\n",
    "    train = train[[\"Class\",\"Sequence\"]]\n",
    "    test = pd.read_csv(TEST)\n",
    "    test =  test[[\"Class\",\"Sequence\"]]\n",
    "    sequences = pd.concat([train,test], axis = 0).Sequence.apply(lambda x: \" \".join(x))\n",
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = max(pd.concat([train[train.Class != \"NonR\"],test[test.Class != \"NonR\"]], axis = 0).Sequence.apply(lambda x: len(x)))\n",
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Sequence\"] = train.Sequence.apply(lambda x: \" \".join(x))\n",
    "X_train = tf.convert_to_tensor(train.Sequence.to_list())\n",
    "y_train = ENCODER.transform(train.Class.to_numpy().reshape(-1,1)).toarray()\n",
    "\n",
    "test[\"Sequence\"] = test.Sequence.apply(lambda x: \" \".join(x))\n",
    "X_test = tf.convert_to_tensor(test.Sequence.to_list())\n",
    "y_test = ENCODER.transform(test.Class.to_numpy().reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()        \n",
    "cnn.add(tf.keras.layers.TextVectorization(max_tokens = 21 + 2, output_mode = \"int\", standardize = None, output_sequence_length = MAX_LEN))\n",
    "cnn.add(tf.keras.layers.Embedding(input_dim = 21 + 2, output_dim = CNN_DIM, input_length = MAX_LEN, mask_zero = True))\n",
    "cnn.add(tf.keras.layers.Conv1D(filters = 512,   kernel_size = 5, activation = \"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPooling1D())\n",
    "cnn.add(tf.keras.layers.Conv1D(filters = 512/2, kernel_size = 5, activation = \"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPooling1D())\n",
    "cnn.add(tf.keras.layers.Conv1D(filters = 512/4, kernel_size = 5, activation = \"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPooling1D())\n",
    "cnn.add(tf.keras.layers.Conv1D(filters = 512/8, kernel_size = 5, activation = \"relu\"))\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "cnn.add(tf.keras.layers.Dense(OUTDIM, activation = 'softmax'))\n",
    "cnn.layers[0].adapt(sequences)\n",
    "cnn.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.001),loss = tf.keras.losses.CategoricalCrossentropy(name = \"CategoricalAccuracy\"),metrics = [tf.metrics.CategoricalAccuracy(name = \"Accuracy\")])\n",
    "cnn_h = cnn.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_same = tf.keras.models.Sequential()        \n",
    "cnn_same.add(tf.keras.layers.TextVectorization(max_tokens = 21 + 2, output_mode = \"int\", standardize = None, output_sequence_length = MAX_LEN))\n",
    "cnn_same.add(tf.keras.layers.Embedding(input_dim = 21 + 2, output_dim = CNNS_DIM, input_length = MAX_LEN, mask_zero = True))\n",
    "cnn_same.add(tf.keras.layers.Conv1D(filters = 512,   kernel_size = 5, activation = \"relu\"))\n",
    "cnn_same.add(tf.keras.layers.MaxPooling1D())\n",
    "cnn_same.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 5, activation = \"relu\"))\n",
    "cnn_same.add(tf.keras.layers.MaxPooling1D())\n",
    "cnn_same.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 5, activation = \"relu\"))\n",
    "cnn_same.add(tf.keras.layers.MaxPooling1D())\n",
    "cnn_same.add(tf.keras.layers.Conv1D(filters = 512, kernel_size = 5, activation = \"relu\"))\n",
    "cnn_same.add(tf.keras.layers.Flatten())\n",
    "cnn_same.add(tf.keras.layers.Dense(OUTDIM, activation = 'softmax'))\n",
    "cnn_same.layers[0].adapt(sequences)\n",
    "cnn_same.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.001),loss = tf.keras.losses.CategoricalCrossentropy(name = \"CategoricalAccuracy\"),metrics = [tf.metrics.CategoricalAccuracy(name = \"Accuracy\")])\n",
    "cnn_same_h = cnn_same.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru = tf.keras.models.Sequential()        \n",
    "gru.add(tf.keras.layers.TextVectorization(max_tokens = 21 + 2, output_mode = \"int\", standardize = None, output_sequence_length = MAX_LEN))\n",
    "gru.add(tf.keras.layers.Embedding(input_dim = 21 + 2, output_dim = GRU_DIM, input_length = MAX_LEN, mask_zero = True))\n",
    "gru.add(tf.keras.layers.GRU(512,return_sequences=True))\n",
    "gru.add(tf.keras.layers.GRU(512,return_sequences=True))\n",
    "gru.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "gru.add(tf.keras.layers.Dense(OUTDIM, activation = 'sigmoid'))\n",
    "gru.layers[0].adapt(sequences)\n",
    "gru.compile(optimizer = tf.optimizers.Adam(learning_rate = 0.001),loss = tf.keras.losses.BinaryCrossentropy(name = \"BinaryCrossentropy\"),metrics = [tf.metrics.BinaryAccuracy(name = \"BinaryAccuracy\")])\n",
    "gru_hist = gru.fit(X_train, y_train, epochs=EPOCHS, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.lineplot(x = list(range(1,EPOCHS+1)), y = gru_hist.history[\"loss\"], label = \"GRU-Training\", c=\"blue\")\n",
    "sn.lineplot(x = list(range(1,EPOCHS+1)), y = gru_hist.history[\"val_loss\"], label = \"GRU-Validation\", c=\"blue\", linestyle='dashed')\n",
    "\n",
    "sn.lineplot(x = list(range(1,EPOCHS+1)), y = cnn_same_h.history[\"loss\"], label = \"CNNS-Training\", c=\"red\")\n",
    "sn.lineplot(x = list(range(1,EPOCHS+1)), y = cnn_same_h.history[\"val_loss\"], label = \"CNNS-Validation\", c=\"red\", linestyle='dashed')\n",
    "\n",
    "sn.lineplot(x = list(range(1,EPOCHS+1)), y = cnn_h.history[\"loss\"], label = \"CNN-Training\", c = \"green\")\n",
    "sn.lineplot(x = list(range(1,EPOCHS+1)), y = cnn_h.history[\"val_loss\"], label = \"CNN-Validation\",c = \"green\", linestyle='dashed')\n",
    "\n",
    "\n",
    "plt.title(\"Model Convergence\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"CategoricalCrossentropy\")\n",
    "plt.savefig(f\"{OUTPUTDIR}/all-uniprot-align.svg\", format=\"SVG\", bbox_inches='tight')\n",
    "plt.savefig(f\"{OUTPUTDIR}/all-uniprot-align.png\", format=\"PNG\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OUTPUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(f\"{OUTPUTDIR}/cnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_same.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_same.save(f\"{OUTPUTDIR}/cnn-same\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gru.save(f\"{OUTPUTDIR}/gru\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsea",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
